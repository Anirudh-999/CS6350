{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1650695,"sourceType":"datasetVersion","datasetId":976194}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/anirudhrangu/cs6350?scriptVersionId=270734723\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# KITTI Dataset Visualization\n\nIn this step, we:\n1. **Load** stereo image pairs (left and right camera) from the KITTI dataset.\n2. **Read** the corresponding calibration text file to extract:\n   - The **rotation matrix (R)**\n   - The **translation vector (t)**\n3. **Display** a few sample images with their R and t values to confirm that data loading is correct.\n\nThis helps us ensure that the image data and ground-truth poses are aligned before moving into keypoint detection and pose estimation.\n","metadata":{}},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom mpl_toolkits.mplot3d import Axes3D","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_dir = \"/kaggle/input/kitti-dataset\"  \nleft_dir = os.path.join(base_dir, \"data_object_image_2/training/image_2/000000.png\")  # left camera images\npose_file = os.path.join(base_dir, \"data_object_label_2/training/label_2/000000.txt\")  # pose file","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndef parse_kitti_label_file(label_path):\n\n    columns = [\n        \"type\", \"truncated\", \"occluded\", \"alpha\",\n        \"bbox_xmin\", \"bbox_ymin\", \"bbox_xmax\", \"bbox_ymax\",\n        \"height\", \"width\", \"length\",\n        \"pos_x\", \"pos_y\", \"pos_z\",\n        \"rotation_y\"\n    ]\n    \n    data = []\n\n    try:\n        with open(label_path, 'r') as f:\n            for line in f:\n                parts = line.strip().split()\n                if parts[0] == \"DontCare\":\n                    continue  # skip non-labeled regions\n                values = parts[:15]  # ensure only 15 fields are taken\n                data.append(values)\n        \n        # Convert to DataFrame\n        df = pd.DataFrame(data, columns=columns)\n        \n        # Convert numeric columns to float\n        for col in columns[1:]:\n            df[col] = df[col].astype(float)\n        \n        return df\n    except:\n        with exception as e:\n            print(\"error:\", e)\n\n# Example usage:\nlabel_file = \"/kaggle/input/kitti-dataset/data_object_label_2/training/label_2/000001.txt\"\ndf = parse_kitti_label_file(label_file)\nprint(df.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"def preprocess_image(img_path):\n    img_rgb = cv2.imread(img_path)\n    \n    # grayscale\n    gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n    \n    # Gaussian Blur (reduces high-frequency noise)\n    gray = cv2.GaussianBlur(gray, (5, 5), sigmaX=1.0)\n    \n    # CLAHE (Contrast Limited Adaptive Histogram Equalization)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    gray_eq = clahe.apply(gray)\n\n    # Normalize pixel intensities (0â€“255 range)\n    norm = cv2.normalize(gray_eq, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n\n    # edge enhancement to improve feature matching\n    enhanced = cv2.bilateralFilter(norm, d=5, sigmaColor=50, sigmaSpace=50)\n\n    return enhanced","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_disparity(left_img, right_img):\n    \"\"\"\n    Compute disparity map from stereo pair\n    Returns:\n        disparity: Disparity map (float32)\n    \"\"\"\n    min_disp = 0\n    num_disp = 128  \n    block_size = 5\n\n    stereo = cv2.StereoSGBM_create(\n        minDisparity=min_disp,\n        numDisparities=num_disp,\n        blockSize=block_size,\n        P1=8 * 3 * block_size ** 2,\n        P2=32 * 3 * block_size ** 2,\n        disp12MaxDiff=1,\n        uniquenessRatio=10,\n        speckleWindowSize=100,\n        speckleRange=32,\n        preFilterCap=63,\n        mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n    )\n\n    disparity = stereo.compute(left_img, right_img).astype(np.float32) / 16.0\n    \n    return disparity","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_depth(disparity, P1, P2):\n    \"\"\"\n    Convert disparity to depth using calibration matrices\n    Returns:\n        depth_map: Depth in meters\n        focal_length: Focal length in pixels\n        baseline: Baseline in meters\n    \"\"\"\n    \n    # Extract focal length from P1\n    focal_length = P1[0, 0]\n    \n    # Calculate baseline from P1 and P2\n    # baseline = |t_x| / f_x\n    baseline = abs(P2[0, 3] - P1[0, 3]) / focal_length\n    \n    print(f\"Focal length: {focal_length:.2f} px\")\n    print(f\"Baseline: {baseline:.4f} m\")\n    \n    # Compute depth: Z = (f * B) / d\n    # Avoid division by zero\n    disparity[disparity <= 0] = 0.1\n    depth_map = (focal_length * baseline) / disparity\n    \n    # Filter unrealistic depth values\n    depth_map[depth_map > 100] = 100  # Max 100 meters\n    depth_map[depth_map < 0] = 0\n    \n    return depth_map, focal_length, baseline","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def backproject_to_3d(depth_map, P1, mask=None):\n    \"\"\"\n    Convert depth map to 3D point cloud\n    Returns:\n        points_3d: Nx3 array of 3D points\n    \"\"\"\n    \n    # Extract camera intrinsics\n    fx = P1[0, 0]\n    fy = P1[1, 1]\n    cx = P1[0, 2]\n    cy = P1[1, 2]\n    \n    h, w = depth_map.shape\n    \n    # Create pixel coordinate grid\n    u, v = np.meshgrid(np.arange(w), np.arange(h))\n    \n    # Back-project to 3D\n    X = (u - cx) * depth_map / fx\n    Y = (v - cy) * depth_map / fy\n    Z = depth_map\n    \n    # Stack into point cloud\n    points_3d = np.stack((X, Y, Z), axis=-1)\n    \n    # Apply mask if provided\n    if mask is not None:\n        points_3d = points_3d[mask > 0]\n    else:\n        points_3d = points_3d.reshape(-1, 3)\n    \n    # Filter invalid points\n    valid = (points_3d[:, 2] > 0) & (points_3d[:, 2] < 100)\n    points_3d = points_3d[valid]\n    \n    return points_3d\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def detect_and_match_features(img1, img2, method='ORB', max_features=5000):\n    \"\"\"\n    Detect and match features between two frames\n    Returns:\n        pts1: Matched keypoints in image 1\n        pts2: Matched keypoints in image 2\n        matches: Good matches\n    \"\"\"\n    \n    if method == 'SIFT':\n        detector = cv2.SIFT_create(nfeatures=max_features)\n        matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n    else:  # ORB\n        detector = cv2.ORB_create(nfeatures=max_features)\n        matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n    \n    # Detect keypoints and compute descriptors\n    kp1, des1 = detector.detectAndCompute(img1, None)\n    kp2, des2 = detector.detectAndCompute(img2, None)\n    \n    if des1 is None or des2 is None or len(kp1) < 10 or len(kp2) < 10:\n        print(\"Insufficient features detected\")\n        return None, None, None\n    \n    # Match using KNN\n    matches = matcher.knnMatch(des1, des2, k=2)\n    \n    # Apply Lowe's ratio test\n    good_matches = []\n    for match_pair in matches:\n        if len(match_pair) == 2:\n            m, n = match_pair\n            if m.distance < 0.75 * n.distance:\n                good_matches.append(m)\n    \n    print(f\"Found {len(good_matches)} good matches\")\n    \n    if len(good_matches) < 8:\n        print(\"Not enough good matches\")\n        return None, None, None\n    \n    # Extract matched point coordinates\n    pts1 = np.float32([kp1[m.queryIdx].pt for m in good_matches])\n    pts2 = np.float32([kp2[m.trainIdx].pt for m in good_matches])\n    \n    return pts1, pts2, good_matches","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_3d_points_for_matches(pts2d, depth_map, P1):\n    \"\"\"\n    Get 3D coordinates for matched 2D points\n    \n    Args:\n        pts2d: 2D points in image (Nx2)\n        depth_map: Depth map\n        P1: Camera projection matrix\n    \n    Returns:\n        pts3d: 3D points (Mx3)\n        valid_pts2d: Corresponding valid 2D points (Mx2)\n    \"\"\"\n    \n    fx = P1[0, 0]\n    fy = P1[1, 1]\n    cx = P1[0, 2]\n    cy = P1[1, 2]\n    \n    pts3d = []\n    valid_pts2d = []\n    \n    for pt in pts2d:\n        u, v = int(pt[0]), int(pt[1])\n        \n        # Check bounds\n        if 0 <= u < depth_map.shape[1] and 0 <= v < depth_map.shape[0]:\n            Z = depth_map[v, u]\n            \n            # Valid depth check\n            if 0.1 < Z < 100:\n                X = (u - cx) * Z / fx\n                Y = (v - cy) * Z / fy\n                pts3d.append([X, Y, Z])\n                valid_pts2d.append(pt)\n    \n    return np.array(pts3d, dtype=np.float32), np.array(valid_pts2d, dtype=np.float32)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def estimate_pose_pnp(pts3d_prev, pts2d_curr, K, dist_coeffs=None):\n    \"\"\"\n    Estimate camera pose using 3D-2D correspondences\n    \n    Args:\n        pts3d_prev: 3D points from previous frame (Nx3)\n        pts2d_curr: 2D points in current frame (Nx2)\n        K: Camera intrinsic matrix (3x3)\n        dist_coeffs: Distortion coefficients (None for KITTI rectified)\n    \n    Returns:\n        R: Rotation matrix (3x3)\n        t: Translation vector (3x1)\n        inliers: Inlier mask\n    \"\"\"\n    \n    if len(pts3d_prev) < 6:\n        print(\"Not enough 3D-2D correspondences\")\n        return None, None, None\n    \n    # Solve PnP with RANSAC\n    success, rvec, tvec, inliers = cv2.solvePnPRansac(\n        pts3d_prev,\n        pts2d_curr,\n        K,\n        dist_coeffs,\n        iterationsCount=1000,\n        reprojectionError=2.0,\n        confidence=0.99,\n        flags=cv2.SOLVEPNP_ITERATIVE\n    )\n    \n    if not success or inliers is None or len(inliers) < 6:\n        print(\"Pose estimation failed\")\n        return None, None, None\n    \n    # Convert rotation vector to matrix\n    R, _ = cv2.Rodrigues(rvec)\n    t = tvec\n    \n    print(f\"PnP inliers: {len(inliers)}/{len(pts3d_prev)}\")\n    \n    return R, t, inliers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def estimate_pose_essential(pts1, pts2, K):\n    \"\"\"\n    Estimate pose using Essential Matrix (2D-2D correspondences)\n    \n    Args:\n        pts1: 2D points in frame 1\n        pts2: 2D points in frame 2\n        K: Camera intrinsic matrix\n    \n    Returns:\n        R: Rotation matrix\n        t: Translation vector\n        mask: Inlier mask\n    \"\"\"\n    \n    # Find Essential Matrix\n    E, mask = cv2.findEssentialMat(\n        pts1, pts2, K,\n        method=cv2.RANSAC,\n        prob=0.999,\n        threshold=1.0\n    )\n    \n    if E is None:\n        print(\"Essential matrix computation failed\")\n        return None, None, None\n    \n    # Recover pose\n    _, R, t, mask = cv2.recoverPose(E, pts1, pts2, K, mask=mask)\n    \n    inliers = np.sum(mask)\n    print(f\"Essential matrix inliers: {inliers}/{len(pts1)}\")\n    \n    return R, t, mask\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class StereoVisualOdometry:\n    \"\"\"Complete stereo visual odometry pipeline\"\"\"\n    \n    def __init__(self, P1, P2):\n        \"\"\"\n        Initialize visual odometry\n        \n        Args:\n            P1: Left camera projection matrix (3x4)\n            P2: Right camera projection matrix (3x4)\n        \"\"\"\n        self.P1 = P1\n        self.P2 = P2\n        self.K = P1[:, :3]  # Intrinsic matrix\n        \n        # Calculate baseline\n        self.baseline = abs(P2[0, 3] - P1[0, 3]) / self.K[0, 0]\n        \n        # Current pose (world to camera transformation)\n        self.R_total = np.eye(3)\n        self.t_total = np.zeros((3, 1))\n        \n        # Trajectory storage\n        self.trajectory = [np.zeros(3)]\n        \n        # Previous frame data\n        self.prev_left = None\n        self.prev_depth = None\n        \n    def process_stereo_pair(self, left_img, right_img):\n        \"\"\"\n        Process a stereo pair and update pose\n        \n        Args:\n            left_img: Preprocessed left grayscale image\n            right_img: Preprocessed right grayscale image\n        \n        Returns:\n            success: Whether pose was successfully estimated\n        \"\"\"\n        \n        # Step 1: Compute disparity and depth\n        print(\"\\n--- Computing disparity ---\")\n        disparity = compute_disparity(left_img, right_img)\n        depth_map, _, _ = compute_depth(disparity, self.P1, self.P2)\n        \n        # First frame: just store\n        if self.prev_left is None:\n            self.prev_left = left_img\n            self.prev_depth = depth_map\n            print(\"First frame initialized\")\n            return True\n        \n        # Step 2: Feature matching between frames\n        print(\"\\n--- Matching features ---\")\n        pts_prev, pts_curr, matches = detect_and_match_features(\n            self.prev_left, left_img, method='ORB'\n        )\n        \n        if pts_prev is None or len(pts_prev) < 8:\n            print(\"Failed: Not enough matches\")\n            return False\n        \n        # Step 3: Get 3D points from previous frame\n        print(\"\\n--- Getting 3D points ---\")\n        pts3d_prev, valid_pts_prev = get_3d_points_for_matches(\n            pts_prev, self.prev_depth, self.P1\n        )\n        \n        if len(pts3d_prev) < 6:\n            print(\"Failed: Not enough 3D points\")\n            return False\n        \n        # Find corresponding current 2D points\n        pts2d_curr = []\n        pts3d_filtered = []\n        \n        for i, prev_pt in enumerate(pts_prev):\n            for j, valid_pt in enumerate(valid_pts_prev):\n                if np.allclose(prev_pt, valid_pt, atol=0.1):\n                    pts2d_curr.append(pts_curr[i])\n                    pts3d_filtered.append(pts3d_prev[j])\n                    break\n        \n        pts2d_curr = np.array(pts2d_curr, dtype=np.float32)\n        pts3d_filtered = np.array(pts3d_filtered, dtype=np.float32)\n        \n        if len(pts3d_filtered) < 6:\n            print(\"Failed: Not enough valid correspondences\")\n            return False\n        \n        # Step 4: Estimate relative pose\n        print(\"\\n--- Estimating pose ---\")\n        R_rel, t_rel, inliers = estimate_pose_pnp(\n            pts3d_filtered, pts2d_curr, self.K\n        )\n        \n        if R_rel is None:\n            print(\"Failed: Pose estimation unsuccessful\")\n            return False\n        \n        # Step 5: Update cumulative pose\n        # T_new = T_old * T_rel\n        self.t_total = self.t_total + self.R_total @ t_rel\n        self.R_total = self.R_total @ R_rel\n        \n        # Store position\n        position = self.t_total.flatten()\n        self.trajectory.append(position.copy())\n        \n        print(f\"Position: x={position[0]:.2f}, y={position[1]:.2f}, z={position[2]:.2f}\")\n        \n        # Update for next iteration\n        self.prev_left = left_img\n        self.prev_depth = depth_map\n        \n        return True\n    \n    def get_trajectory(self):\n        \"\"\"Get trajectory as numpy array\"\"\"\n        return np.array(self.trajectory)\n    \n    def get_current_pose(self):\n        \"\"\"Get current pose (R, t)\"\"\"\n        return self.R_total, self.t_total\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_disparity(disparity):\n    \"\"\"Visualize disparity map\"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.imshow(disparity, cmap='jet')\n    plt.colorbar(label='Disparity (pixels)')\n    plt.title('Disparity Map')\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\ndef visualize_depth(depth_map):\n    \"\"\"Visualize depth map\"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.imshow(depth_map, cmap='plasma', vmin=0, vmax=50)\n    plt.colorbar(label='Depth (meters)')\n    plt.title('Depth Map')\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\ndef visualize_matches(img1, img2, pts1, pts2, num_show=50):\n    \"\"\"Visualize feature matches\"\"\"\n    h1, w1 = img1.shape\n    h2, w2 = img2.shape\n    \n    # Create side-by-side image\n    combined = np.zeros((max(h1, h2), w1 + w2), dtype=np.uint8)\n    combined[:h1, :w1] = img1\n    combined[:h2, w1:] = img2\n    combined = cv2.cvtColor(combined, cv2.COLOR_GRAY2BGR)\n    \n    # Draw matches\n    num_show = min(num_show, len(pts1))\n    for i in range(num_show):\n        pt1 = tuple(pts1[i].astype(int))\n        pt2 = tuple((pts2[i] + [w1, 0]).astype(int))\n        color = tuple(np.random.randint(0, 255, 3).tolist())\n        cv2.circle(combined, pt1, 3, color, -1)\n        cv2.circle(combined, pt2, 3, color, -1)\n        cv2.line(combined, pt1, pt2, color, 1)\n    \n    plt.figure(figsize=(15, 6))\n    plt.imshow(combined)\n    plt.title(f'Feature Matches (showing {num_show})')\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\ndef visualize_trajectory(trajectory, ground_truth=None):\n    \"\"\"Visualize 2D trajectory (top-down view)\"\"\"\n    plt.figure(figsize=(10, 10))\n    \n    traj = np.array(trajectory)\n    plt.plot(traj[:, 0], traj[:, 2], 'b-o', label='Estimated', markersize=3)\n    \n    if ground_truth is not None:\n        gt = np.array(ground_truth)\n        plt.plot(gt[:, 0], gt[:, 2], 'r-', label='Ground Truth', linewidth=2)\n    \n    plt.xlabel('X (meters)')\n    plt.ylabel('Z (meters)')\n    plt.title('Camera Trajectory (Top-Down View)')\n    plt.legend()\n    plt.axis('equal')\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\ndef visualize_3d_points(points_3d, max_points=10000):\n    \"\"\"Visualize 3D point cloud\"\"\"\n    # Subsample if too many points\n    if len(points_3d) > max_points:\n        indices = np.random.choice(len(points_3d), max_points, replace=False)\n        points_3d = points_3d[indices]\n    \n    fig = plt.figure(figsize=(10, 8))\n    ax = fig.add_subplot(111, projection='3d')\n    \n    ax.scatter(points_3d[:, 0], points_3d[:, 1], points_3d[:, 2], \n               c=points_3d[:, 2], cmap='viridis', s=1)\n    \n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    ax.set_title('3D Point Cloud')\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    \"\"\"\n    Main execution pipeline\n    \"\"\"\n    \n    # Parse calibration matrices (your KITTI values)\n    P1 = np.array([\n        [7.070493e+02, 0.000000e+00, 6.040814e+02, -3.797842e+02],\n        [0.000000e+00, 7.070493e+02, 1.805066e+02, 0.000000e+00],\n        [0.000000e+00, 0.000000e+00, 1.000000e+00, 0.000000e+00]\n    ])\n    \n    P2 = np.array([\n        [7.070493e+02, 0.000000e+00, 6.040814e+02, 4.575831e+01],\n        [0.000000e+00, 7.070493e+02, 1.805066e+02, -3.454157e-01],\n        [0.000000e+00, 0.000000e+00, 1.000000e+00, 4.981016e-03]\n    ])\n    \n    print(\"=\" * 60)\n    print(\"STEREO VISUAL ODOMETRY PIPELINE\")\n    print(\"=\" * 60)\n    \n    # Initialize visual odometry\n    vo = StereoVisualOdometry(P1, P2)\n    \n    # Example: Process stereo image sequence\n    # Replace with your actual image loading\n    \n    #for frame_id in range(num_frames):\n    for frame_id in range(1):\n        # Load stereo pair (preprocessed)\n        left_img = preprocess_image(\"/kaggle/input/kitti-dataset/data_object_image_2/training/image_2/000000.png\")\n        right_img = preprocess_image(\"/kaggle/input/kitti-dataset/data_object_image_3/training/image_3/000000.png\")\n        \n        # Process frame\n        print(f\"\\n{'='*60}\")\n        print(f\"FRAME {frame_id}\")\n        print(f\"{'='*60}\")\n        \n        success = vo.process_stereo_pair(left_img, right_img)\n        \n        if not success:\n            print(f\"Frame {frame_id} failed\")\n            continue\n    \n    # Get trajectory\n    trajectory = vo.get_trajectory()\n    \n    # Visualize results\n    visualize_trajectory(trajectory)\n    \n    # Save trajectory\n    np.savetxt('estimated_trajectory.txt', trajectory)\n    print(\"\\nTrajectory saved to 'estimated_trajectory.txt'\")\n    \n    \n    # Demo with single stereo pair\n    print(\"\\nTo use this pipeline:\")\n    print(\"1. Load your preprocessed left and right images\")\n    print(\"2. Initialize StereoVisualOdometry with P1, P2\")\n    print(\"3. Call process_stereo_pair() for each frame pair\")\n    print(\"4. Use get_trajectory() to retrieve camera path\")\n    print(\"5. Visualize with provided visualization functions\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}